import jsonimport numpy as npimport events as eimport settings as simport sysimport osINVALID_ACTION = 6DROP_BOMB = 4KILLED_SELF = 13GOT_KILLED = 14def write_dict_to_file(self):    """    JSON to structure data as a dictionary    Args:        self:    """    with open('q_table.json', 'w') as output:        output.write(json.dumps(self.q_table))        output.close()def read_dict_from_file(self):    """    Read q_table dictionary from JSON    Args:        self:    """    with open('q_table.json', 'r') as input:        self.q_table = json.load(input)        input.close()def training_radius(self):    """    Radius is set according to coins instead of crates for training with coins    Args:        self:    """    coins = np.array(self.game_state['coins'])    agent = np.array([self.game_state['self'][0], self.game_state['self'][1]])    self.radius = max(min(np.linalg.norm(agent - coins, axis=1)), 4 * np.sqrt(2))def real_radius(self):    """    Calculate searching radius according to crates    Args:        self:    """    crates = np.array(np.where(self.game_state['field'] == 1))    crates = crates.reshape(len(crates[0]), 2)    agent = np.array([self.game_state['self'][0], self.game_state['self'][1]])    self.radius = max(min(np.linalg.norm(agent - crates, axis=1)), 4 * np.sqrt(2))def state_to_str(state):    """    Changes list with states to string    Strings are used as keys for the q_table dictionary    Args:        state:    Returns:    """    return ", ".join(state)def dangerous_bombs(self, pos):    """    Identifies bombs that put agent in danger    Args:        self:        pos: coordinates of tile in danger    Returns: list of bombs that put agent in danger    """    # Bombs position    bombs = np.array([x, y] for x, y, t in self.game_state['bombs'] if (x == pos[0][0] or y == pos[0][1]))    if len(bombs):        bombs_distance = np.linalg.norm(bombs - pos, axis=1)        bombs = bombs[bombs_distance < 4]        if len(bombs):            # Distance from dangerous bombs            dangerous_bombs = bombs_distance[bombs_distance < 4]            # Determine direction of the bomb            temp = np.sign(bombs - pos)            indices = []            for i, direction in enumerate(temp):                # clear is a boolean variable to indicate if tile is clear                temp_pos, clear = pos, False                for _ in range(int(dangerous_bombs)):                    temp_pos = np.array([temp_pos[0] + direction[0], temp_pos[1] + direction[1]])                    if self.game_state['field'][temp_pos[0], temp_pos[1]] == -1:                        # There is a bomb between wall and agent                        clear = True                        break                if not clear:                    # Agent is in danger                    indices.append(i)            return bombs[np.array(indices)] if len(indices) else []    return []def find_priority(self, current_state, cand_name):    """    Score among possible movements is computed.    Score consists on a weighted sum of crate, coin, and enemy considering the squared relative distance from    the checked position.    Args:        self:        current_state: current built state        cand_name: Conditional state (empty, danger1)    Returns:    """    candidates = np.array(current_state)[:4]    candidates = np.where(candidates == cand_name)[0]    if not len(candidates):        return    # Calculate scores    scores, best_indices = np.zeros(len(candidates)), []    for i, index in enumerate(candidates):        col, row = self.changes_point_in_dir[index]        temp_pos = np.array([self.curr_pos[0] + col, self.curr_pos[1] + row])        if cand_name == 'danger':            bombs_cord = dangerous_bombs(self, temp_pos)            if len(bombs_cord):                distance = np.linalg.norm(temp_pos - bombs_cord, axis=1)                closest_bomb = bombs_cord[np.argmin(distance)]                if min(distance) < np.linalg.norm(self.curr_pos - closest_bomb):                    scores[i] = np.inf                    continue                else:                    scores[i] += len(bombs_cord) * -20                    bombs = np.array(self.game_state['bombs'])[:, [0, 1]]                    distance = np.linalg.norm(temp_pos - bombs)                    scores[i] += np.sum(-20 * (1 / distance))        for object_name in self.weights.keys():            if object_name == 'field':                objects = np.array(np.where(self.game_state['field'] == 1))                objects = objects.transpose()            elif object_name == 'coins':                objects = np.array(self.game_state['coins'])            else:                enemies = np.array(self.game_state['others'])                objects = enemies[:, [0, 1]].astype(int) if len(enemies) else []            if len(objects):                distance = np.linalg.norm(temp_pos - objects, axis=1)                indices = np.where(distance <= self.radius)[0]                rel_distance = distance[indices]                if len(rel_distance):                    temp = np.ones(len(rel_distance)) * self.weights[object_name]                    scores[i] += np.sum(temp / ((rel_distance + 1) ** 2))    # Choose the best route according to the scores.    if cand_name == 'danger':        scores = np.array(scores)        candidates = candidates[scores != np.inf]        scores = scores[scores != np.inf]        if len(scores) == 0:            return        best_indices = candidates[np.where(np.array(scores) == max(scores))[0]]    else:        best_indices = candidates[np.where(np.array(scores) == max(scores))[0]]    j = np.random.choice(best_indices)    current_state[j] = 'priority'def danger_boolean(self, pos):    """    Determines if agent is in danger    Args:        self:        pos: checked tilw    Returns:    """    bombs = dangerous_bombs(self, pos)    if len(bombs):        bombs_dis = np.linalg.norm(bombs - pos, axis=1)        return len(np.where(bombs_dis < 4)[0]) > 0    return Falsedef clear_path(self, idx, curr_pos, counter=4):    """    For each possible direction (empty tiles), a path of 4 tiles is recalculated in order to escape from a bomb.    This is done recursively.    Args:        agent:        idx: direction (0: left, 1: up, 2: right, 3: down)        curr_pos: current position of the agent        counter: number of tiles left to check in path.    Returns: boolean - 1 if the path is clear, 0 if not    """    try:        dir_var = self.changes_point_in_dir[idx]        new_pos = np.array([curr_pos[0] + dir_var[0], curr_pos[1] + dir_var[1]])        if counter == 0:            return True        elif self.game_state['field'][new_pos[0], new_pos[1]] != 0:            return False        elif self.game_state['explosion_map'][new_pos[0], new_pos[1]] > 2:            return False        elif len(self.game_state["others"]):            for enemy in self.game_state["others"]:                if enemy[0] == new_pos[0] and enemy[1] == new_pos[1]:                    return False        # Search for an escape rout        for direction in [0, 1, 2, 3]:            temp_dir_var = self.changes_point_in_dir[direction]            if np.abs(direction - idx) != 2 and direction != idx and self.game_state['field'][                new_pos[0] + temp_dir_var[0], new_pos[1] + temp_dir_var[1]] == 0:                return True        # Next tile in path        return clear_path(self, idx, new_pos, counter - 1)    except Exception as ex:        exc_type, exc_obj, exc_tb = sys.exc_info()        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]        print("Reward_update: ", e, exc_type, fname, exc_tb.tb_lineno)        bul = 5def setup(self):    self.curr_state = None    self.prev_state = None    self.episode_rewards_log = dict()    # ACTIONS AND EVENTS    self.actions = ['LEFT', 'UP', 'RIGHT', 'DOWN', 'BOMB', 'WAIT']  # Actions in the needed order.    self.changes_point_in_dir = {'left': [-1, 0], 'up': [0, -1], 'right': [1, 0],                                 'down': [0, 1], 0: [-1, 0], 1: [0, -1], 2: [1, 0], 3: [0, 1]}    self.relevant_events = [0, 1, 2, 3, 4, 6, 7, 14]    self.events = {0: 0, 1: 2, 2: 1, 3: 3, 7: 4, 4: 5, 6: 6, 14: 14}    self.rewards = {'bomb': -70, 'danger1': -40, 'coin': 80, 'priority': 60, 'enemy': -10,                    'wall': -70, 'empty': -10, 'danger2': -30, 'danger3': -20, 'invalid': -70, 'goal_action': 80,                    'dead': -70, 'danger': -40, 'crate': -70}    self.train = False  # Todo - check what is that for.    # HYPERPARAMETERS    self.gamma = 0.65    self.alpha = 0.25    # Epsilon-greedy policy    self.epsilon = 1.0    self.epsilon_min = 0.20  # Relatively high minimum eps would prevent overfitting    self.epsilon_decay = 0.9995    # self.radius permits to look for priorities nearby. self.weights establish priorities for the agent    self.radius = None    self.radius_incrementer = 0.1  # Each turn, radius is incremented by 0.05    self.weights = {'field': 1, 'field': 30, 'others': 1}  # Weights for each object on the board.    # Load Q_table:    try:  # If q_table.json file doesn't exit        read_dict_from_file(self)    except Exception as ex:        self.q_table = dict()def act(self):    """    Args:        self:    Returns:    """    self.logger.info('Pick action according to pressed key')    # Epsilon-greedy policy    if self.game_state['step'] == 1 and self.radius is None:        training_radius(self) if s.crate_density == 0 else real_radius(self)    # First step: find the current state.    self.curr_state = find_state(self)    # Second step:    string = state_to_str(self.curr_state)    if self.train_flag:        action_rewards = np.array(self.q_table[string])        if 0 in action_rewards:            indices = np.where(action_rewards == 0)[0]            action = np.random.choice(indices)        # Epsilon-Greedy Policy: Epsilon is updated after every finished round while training.        # Exploration- Exploitation Policy: epsilon_min permits the agent to still try random actions for different states        elif np.random.uniform(0, 1) < self.epsilon:            # Speed-up training: agent is encouraged to do an action for a state that has no action info            action = np.random.randint(0, 6)        else:            action = np.argmax(self.q_table[string])        # Prepared q_table should be used when not training    else:        try:            # Exception for not known states. A random action is chosen            knowledge_list = np.array(self.q_table[state_to_str(self.curr_state)])            best_indices = np.where(knowledge_list == max(knowledge_list))[0]            action = np.random.choice(best_indices)        except KeyError as ke:            action = np.random.randint(0, 6)    # Set action and change radius.    self.radius += self.radius_incrementer    self.next_action = self.actions[action]def find_state(self):    """    1st.: In each iteration, the state of each tile in all possible directions is determined and analyzed    2nd.: Priority policy: If no imminent danger, best move will be calculated    Policies: Grading of each empty direction by number of objects on field taking into consideration their    respective weights and distance from the agent.    Direction's value = \Sum_i object_i * weight(object_i) / distance_from_agent(object_i)    Args:        self:    Returns:    """    options = [(-1, 0, 'left'), (0, -1, 'up'), (1, 0, 'right'), (0, 1, 'down'), (0, 0, 'self')]    curr_state = list()    agent_coordinates = np.array([self.game_state['self'][3][0], self.game_state['self'][3][1]])    self.curr_pos = agent_coordinates    for col, row, direction in options:        # Tile to be analyzed        observed_coordinates = np.array([agent_coordinates[0] + col, agent_coordinates[1] + row])        # Check if tile is full or not        if self.game_state['field'][observed_coordinates[0], observed_coordinates[1]] == -1:            curr_state.append('wall')            continue        elif self.game_state['field'][observed_coordinates[0], observed_coordinates[1]] == 1:            curr_state.append('crate')            continue        elif self.game_state['explosions'][observed_coordinates[0], observed_coordinates[1]] > 1:            curr_state.append('wall')            continue        players = np.array(self.game_state['others'])        if len(players):            players_dis = np.linalg.norm(players[:, [0, 1]].astype(int) - observed_coordinates, axis=1)            if len(players[players_dis == 0]):                curr_state.append('enemy')                continue        bombs = np.array([[x, y] for x, y, t in self.game_state['bombs'] if                          (x == observed_coordinates[0] or y == observed_coordinates[1])])        if len(bombs):            bombs_distance = np.linalg.norm(bombs - observed_coordinates, axis=1)            if len(bombs[bombs_distance == 0]) > 0:                curr_state.append('bomb')                continue            if danger_boolean(self, observed_coordinates):                curr_state.append('danger')                continue        if direction == 'self':            curr_state.append('empty')            continue        coins = np.array(self.game_state['coins'])        if len(coins):            coins = np.linalg.norm(coins - observed_coordinates, axis=1)            if len(coins[coins == 0]) > 0:                curr_state.append('coin')                continue        if len(players):            players_dis = np.linalg.norm(players[:, [0, 1]].astype(int) - observed_coordinates, axis=1)            if len(players[players_dis == 0]) > 0:                curr_state.append('enemy')                continue        # Tile is empty        curr_state.append("empty")    # Check for danger    if curr_state[4] == 'bomb':        empty = np.where(np.array(curr_state)[:4] == 'danger')[0]        for j in empty:            if not clear_path(self, j, self.curr_pos):                curr_state[j] = 'wall'    if not ('enemy' in curr_state[:4] and self.game_state['self'][2] == 1) and 'coin' not in curr_state[:4]:        # Dead end when dropping a bomb is not a priority        if 'crate' in curr_state[:4] and self.game_state["self"][2]:            empty = np.array(curr_state)[:4]            empty = np.where(empty == 'empty')[0]            for j in empty:                if not clear_path(self, j, self.curr_pos):                    curr_state[j] = 'wall'        # Direction to empty tile has priority        elif 'empty' in curr_state[:4]:            find_priority(self, curr_state, 'empty')        # If there is no empty tile around us, but there is danger, we should try and find out        # If we still should move into it, another one, or wait in our place.        elif 'danger' in curr_state[:4]:            find_priority(self, curr_state, "danger")    # Bomb action is possible    curr_state.append(str(self.game_state["self"][2] == 0))    # If state is not in q_table, add it with a list of 6 0's.    string_state = state_to_str(curr_state)    try:        self.q_table[string_state]    except Exception as ex:        self.q_table[string_state] = list(np.zeros(6))    return curr_statedef update_reward(self):    """    Allow intermediate rewards based on game events.    Args:        self:    Returns:    """    self.logger.debug(f'Encountered {len(self.events)} game event(s)')    # Restart round log    if self.game_state['step'] == 1:        self.round_rewards_log = dict()        self.train_flag = True        training_radius(self) if s.crate_density == 0 else real_radius(self)    # No action has been made.    if len(self.events) == 0:        return    self.curr_pos = np.array([self.game_state["self"][0], self.game_state["self"][1]])    state_string = state_to_str(self.curr_state)    # First step: We created a list of relevant actions from given events    action, reward_update = 0, 0    for event in self.events:        # Move has been made:        if event not in self.relevant_actions:            continue        # Translation of game settings        action = self.actions.index(self.next_action)        if event == GOT_KILLED:            reward_update += self.rewards['dead']        if event == INVALID_ACTION:            action = self.actions.index(self.next_action)            if self.curr_state[action] == 'priority':                reward_update += self.rewards[self.curr_state[action]]            else:                reward_update += self.rewards['invalid']        if action < DROP_BOMB:            curr_action = self.curr_state[action]            reward_update += self.rewards[curr_action]        elif action == DROP_BOMB:  # action == 4            if 'crate' in self.curr_state[:4] or 'enemy' in self.curr_state[:4]:                reward_update += self.rewards['goal_action']        else:            reward_update += -5 if self.curr_state[4] != 'danger' else self.rewards['bomb']        try:            # Implementation Q-Learning algorithm            prev_state_string = state_to_str(self.prev_state)            alpha, gamma = self.alpha, self.gamma            Q0sa0 = self.q_table[prev_state_string][self.prev_action]            Qs1a1 = self.rewards['dead'] if GOT_KILLED == self.events[-1] else max(self.q_table[state_string])            curr_reward = Q0sa0 + alpha * (self.prev_reward + gamma * Qs1a1 - Q0sa0)            self.q_table[prev_state_string][self.prev_action] = curr_reward        except Exception as ex:            # No previous steps are listed            pass        self.prev_action = action        self.prev_reward = reward_update        self.prev_state = self.curr_statedef end_of_round(self):    """    Called at the end of each game or when the agent died to hand out final rewards.    This is similar to reward_update. self.events will contain all events that    occurred during your agent's final step.    This is *one* of the places where you could update your agent.    This is also a good place to store an agent that you updated.    :param self: The same object that is passed to all of your callbacks.    """    self.logger.debug(f'Encountered {len(self.events)} game event(s) in final step')    # Update reward    update_reward(self)    update_reward(self)    # Reduce epsilon    if self.epsilon >= self.epsilon_min:        self.epsilon *= self.epsilon_decay    # For each round, the Q function variables have to be restarted    self.curr_state = None    self.prev_state = None    self.prev_action = None    # Save q_table    write_dict_to_file(self)    if self.number_of_games % 50 == 0:        print("#games = {}".format(self.number_of_games))